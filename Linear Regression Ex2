# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
df = pd.read_csv('salary_data.csv')

# Display column names to verify structure
print("Columns in dataset:", df.columns)

# Clean column names (remove leading/trailing spaces)
df.columns = df.columns.str.strip()

# Check for missing values
print("Missing values:\n", df.isnull().sum())

# Drop rows with missing values in key columns
df.dropna(subset=['Years of Experience', 'Salary'], inplace=True)

# Visualize data
sns.scatterplot(x='Years of Experience', y='Salary', data=df)
plt.title("Years of Experience vs Salary")
plt.xlabel("Years of Experience")
plt.ylabel("Salary (£)")
plt.show()

# Prepare features and target
X = df[['Years of Experience']]
y = df['Salary']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Model parameters
print("Intercept:", model.intercept_)
print("Slope:", model.coef_[0])

# Predict and compare
y_pred = model.predict(X_test)
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print("\nPrediction Comparison:\n", comparison)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Plot regression line
plt.scatter(X_train, y_train, color='blue', label='Training data')
plt.plot(X_train, model.predict(X_train), color='red', label='Regression line')
plt.title("Linear Regression Fit")
plt.xlabel("Years of Experience")
plt.ylabel("Salary (£)")
plt.legend()
plt.show()

# Residuals
residuals = y_test - y_pred
sns.histplot(residuals, kde=True)
plt.title("Residuals Distribution")
plt.xlabel("Residuals")
plt.show()
